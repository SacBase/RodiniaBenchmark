
* cfd
The result of the SAC generated CUDA code has been compared with the orginal Rodina CUDA version and the SAC 
generated sequential code. The results match for most of the data, however, when the result is too small, 
e.g. 10^(-17), the results from CUDA just shows 0 while the result from the other two can still be displayed 
as some number to the power of -17. This could be due to the difference in architectures but it has not been 
investigated further.

To better utilise the GPU off chip memory bandwidth, layouts of the main arrays have been transformed 
from row-wise to column-wise. For example, arrays "varaibles" and "fluxes" were orginally of shape [SIZE, NVAR]. 
In the Rodinia CUDA version and the SAC source code, they have been changed to shape [NVAR, SIZE]. Similarly,
array "normals" have been transformed from shape [SIZE, NNB, NDIM] (i.e. [SIZE, 4, 3]) to shape [NDIM, NNB, SIZE].
These ensures that when threads in the 1D thread block access "normals", consecutive threads are accessing
consequtive array elements along the inner most dimension.

In the Rodinia CUDA version, the major kernel "compute_flux" is invoked with a 1D thread grid along the X 
dimension. Each thread essentially computes 5 different variables (i.e. density, momentumX, momentumY, momentuZ
and energy density). All five variables are store in the same column of array "fluxes". Indeix of the column
corresponds to the thread index. Such array layout ensures that off chip memory accesses are coalesced. However, 
it is very difficult to express the same parallel computation pattern using withloops in SAC. This is because 
withloop does not allow the inner dimension of an array to be computed in parallel. Therefore, in the current 
SAC code we implement the computation as a withloop with 5 partitions. Essentially, each partition computes one 
"row" of array "fluxes". However, this is extremely inefficeint in the following two ways: 1) we have 5 times 
more kernel invocation overheads (the Rodinia CUDA version has only one kernel); 2) a lot of computation is common 
for all 5 partitions. By having 5 separate kernels, we lose the opportunity to reuse intermediate data computed 
those computations as data in the cache is not persistent across different kernel invocations. 

POTENTIAL IMPROVEMENT:
As we have discussed above, the major factor limiting the performance of the SAC gernerated CUDA code is that 
SAC does not allowed inner dimension of a withloop to be parallelised. This is not a simple problem and if that
is to be changed, the semantic of withloop is changed. However, one potential solution is to enable the withloop
fusion optimization (which is diabled by default). If we were to do that, we need to divide the array "fluxes"
into 5 individual 1D arrays (each with shape [SIZE]). Then we can have five individual withloops to compute each
one. And if withloop fusion is enabled, it should be able to put those five withloops into one single multi-
operator withloop. And in the case, the common computaion can be put back together (hopefully!). However, there
are several issues with this approach: 1) the withloop fusion has not been used for ages, so it is very likely
that by turning it on, there will be a lot of bugs. 2) All CUDA backend code were not expecting multi-operator
withloops before. So they all need to be modified to deal with that. Especially the create CUDA kernel phase which
assuems that only one array is the result array.  

UPDATE ON POTENTIAL IMPROVEMENT:
After investigating withloop fusion for the performance problem, we come across another issue specific to this 
program. If we divid fluxes in to 5 individual 1D arrays, the sac2c compiler runs out of memory because we have
too many arguments to a function. So we are not able to experiment with the effect of withloop fusion. However,
we come up with another idea of "partition fusion". Since each partition can be computed in any order, we can 
fuse them together as long as they have the same shapes. Once all fusable partitions are fused, we only need one
CUDA kernel and identical computations of each partition can be shared. Initially, we are thinking of putting
this "partition fusion" optimization at the very end of memory management. The primary reason is that F_wl_assign 
for each partition has been added and this makes the fusion easier. To fuse the partitions, we first need to find
out those partitions that compute the same shape. This implies that it only works with AKS withloop. We then need
to order the partitions based on their lower bounds. The first partition after this ordering will be the one where
all other partitions are fused into. For the code of each partition that are fused into the first partition, a few
variables need to be changed accrodingly:
1) For each withloop ids, it needs to be incremented by a proper amount
2) iv needs to be redefined based on the new ids computed in step 1.
3) wlidx also needs be incremented by a proper amount.
Apart from the variables mentioned above, the rest of the code should be able to fused without any modifications.          
 
Finally, apart from transforming the arrays layout as one major optimization towards generating more efficient
code, the Rodinia CUDA verion utilises constant memory to store small arrays (i.e. ). We have NOT yet performed 
any experiments to see the benefits of using constant memory in this case althought it can potentially be done
pretty easily. 

Certain preliminary performance results have been performed between the Rodinia CUDA version and the SAC generated
code. The result suggests a more than 10x slowdowns of SAC CUDA code due to the inefficiencies discussed earlier.
More detailed performance comparision will be performed later when we have a better (or quick) solution to the 
parallelization problem. Also note that, if we want to compare the SAC generated sequential code with either
Rodinia sequential or openmp code, we cannot simply use the current SAC source to generate sequential code. This
is because the SAC code has been modified to a certain extent to enable it generating better CUDA code, namely
the array layout transformantion. However, while such layout might be good for CUDA, it might not be good for
CPU code. So we need a different version to generate sequential code from (i.e. to convert arrays back to their
original layout as indicated in the Rodinia openmp version). 

All input files are in the ./input directory. Files with names fvcorr.domn.* are inputs to the Rodinia CUDA and 
OpenMP version. Files with names sac_input_* are inputs to the SAC versions. Here * represent the size (number of 
rows) of the input. The first line of each input file contains the total number of total lines in the file. Note 
that currently, the input size is hardcoded in the program by a #define macro. This means that if we want to use 
a different input file with different size, we need to change the #define macro as well. The reason we do this is 
to ensure all arrays in the SAC program is AKS.  

