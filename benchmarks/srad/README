
* SRAD
The final computed result of the SAC generated code (both sequential and CUDA) has been compared with 
the orginal Rodina versions and they seem to match with each other largely (however, due to roundoff 
errors, slight differences on the order of 10^(-5) exist). Also, we need to manaully change the random 
function "SACrandom" in SAC generated code to the stdlib.h random function "rand" for the different 
versions to produce the same results. 

The original Rodinia CUDA version has been modified with two new kernels which do not utilise the shared 
memory at all (with _noshr postfix). The purpose of this is to compare to the performance of kernels with 
and without shared memory on the new Fermi architecture, which features hardware managed L1/L2 cache. The
runtime results show that, in the particular benchmark, utilising shared memory acutally leads to less 
efficient code. This is mainly because the benefits of small amount of spatial data reuse is nullified by 
the extra complexities added to the kernels (loading data into and stroing data from shared memory). 

The SAC generated sequential code is faster than the Rodinia sequential code (i.e. withou OPEN defined).
This is mainly because of the withloop folding performed by the SAC compiler. 

The SAC generated CUDA code is 2.6x faster than the Rodinia CUDA code (here we refer to the faster no 
shared memory version). This is mainly because, in SAC, we expressed the loop nest at the begining of the 
main stepping loop as fold withloops (i.e. the loop nest to compute the sum and sum of squares of the 
elements of the upper left 128x182 subarray of array J). These two new fold withloops can be cudarized by 
the partial folding optimization and only two small intermediate arrays storing the partial results 
need to be transfer between host and device during each iteration of the main stepping loop. In
contrary to that, the Rodinia CUDA version uses an ordinary for loop nest which is not cudarized.
This means that during each iteration of the stepping loop, the array J needs to be transferred
twice: once at the end of the loop from device to host so that it can be accessed by the loop nest
at the beginging of the next iteration and the other at the begining of the loop from host to device
so that it can be accessed by the following kernels. Since array J is significantly larger than 
the intermediate arrays we transfer in the SAC generated code, the total execution time takes much
longer. However, we might consider to  cudarize the loop nest in the Ridinia CUDA version too to make
a fair comparision.    

