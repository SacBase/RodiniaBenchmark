use StdIO: all;
use Array: all;
//use Constants: {randmax};
use Random: all;
use Math: all;

/*
 * The following 'defines' are used:
 *   FLOAT:  If defined, use single precision floating point data
 *   SEQ:    If defined, use sequential implemenation, i.e. for loops
 *   ERRSUM: If defined, compute error sum
 *   LOOP:   If defined, perform back propagation in a loop
 *   OUTPUT: If defeind, print entire array after computation
 */

#ifdef FLOAT
#define DBL float
#define TOD tof
#else
#define DBL double
#define TOD tod
#endif

#define IN     65536
#define HIDDEN 16
#define OUT    1

#define IN1     65537
#define HIDDEN1 17
#define OUT1    2

#define ITER 20

#define ETA      TOD(0.3)
#define MOMENTUM TOD(0.3)

#define ABS(x) (((x) > TOD(0.0)) ? (x) : (-(x)))

inline
DBL[.,.] bpnn_randomize_weights( DBL[.,.] w)
{
  m = shape(w)[0]-1;
  n = shape(w)[1]-1;

  for (i = 0; i <= m; i++) {
    for (j = 0; j <= n; j++) {
     w[i,j] = TOD(random())/TOD(randmax());
    }
  }
  return( w);
}

inline
DBL[.] bpnn_randomize_row( DBL[.] w)
{
  m = shape(w)[0]-1;
  for (i = 0; i <= m; i++) {
    w[i] = TOD(0.1);
  }
  return( w);
}

inline
DBL[.,.] bpnn_zero_weights( DBL[.,.] w)
{
  m = shape(w)[0]-1;
  n = shape(w)[1]-1;

  for (i = 0; i <= m; i++) {
    for (j = 0; j <= n; j++) {
      w[i, j] = TOD(0.0);
    }
  }
  return( w);
}

inline
DBL[.] load( DBL[.] w)
{
  m = shape(w)[0]-1;

  for (i = 1; i <= m; i++) {
    w[i] =  TOD(random())/TOD(randmax());
  }
  return( w);
}

inline
DBL[.], DBL[.] bpnn_layerforward_sequential(DBL[.] l1, DBL[.] l2, DBL[.,.] conn)
{
  n1 = shape(l1)[0]-1;
  n2 = shape(l2)[0]-1;

  l1[0] = TOD(1.0); 

  for( j = 1; j <= n2; j++) {
    tmp = TOD(0.0);
    for( k = 0; k <= n1; k++) {
      tmp += conn[k,j]*l1[k];
    }
    l2[j] = (TOD(1.0) / (TOD(1.0) + exp(-tmp))); 
  }

  return( l1, l2);
}

inline
DBL[.], DBL[.] bpnn_layerforward_large(DBL[.] l1, DBL[.] l2, DBL[.,.] conn)
{
  n1 = shape(l1)[0]-1;
  n2 = shape(l2)[0]-1;

/*
  l1[0] = TOD(1.0);
*/

  tmp = with {
          ( [0] <= iv=[k] < [n1+1]) {
            res = conn[k]*l1[k];
          }:res;
        }:fold(+, genarray([n2+1],TOD(0.0)));

  l2 = with {
          ([0] <= iv < shape(tmp)):(TOD(1.0) / (TOD(1.0) + exp(-tmp[iv])));
        }:modarray(tmp);

  return( l1, l2);
}

inline
DBL[.], DBL[.] bpnn_layerforward_small(DBL[.] l1, DBL[.] l2, DBL[.,.] conn)
{
  n1 = shape(l1)[0]-1;
  n2 = shape(l2)[0]-1;
/*
  l1[0] = TOD(1.0);
*/
  l2 = with {
         ( [1] <= iv=[j] < [n2+1]) {
           tmp = with {
                   ( [0] <= iv=[k] < [n1+1]):conn[k, j]*l1[k];
                 }:fold(+, TOD(0.0));
           res = (TOD(1.0) / (TOD(1.0) + exp(-tmp)));
         }:res;
       }:modarray(l2);

  return( l1, l2);
}

inline
DBL, DBL[.] bpnn_output_error_sequential(DBL[.] delta, DBL[.] target, DBL[.] output)
{
  nj = shape(delta)[0]-1;

  errsum = TOD(0.0);
  for (j = 1; j <= nj; j++) {
    o = output[j];
    t = target[j];
    delta[j] = o * (TOD(1.0) - o) * (t - o);
    errsum = errsum + ABS(delta[j]);
  }

  return( errsum, delta);
}

inline
DBL, DBL[.] bpnn_output_error(DBL[.] delta, DBL[.] target, DBL[.] output)
{
  nj = shape(delta)[0]-1;

#ifdef ERRSUM
  errsum, delta =
    with {
      ( [1] <= iv=[j] < [nj+1]) {
        o = output[j];
        t = target[j];
        res2 = o * (TOD(1.0) - o) * (t - o);
        res1 = ABS( res2);
      }:(res1, res2);
    }:(fold(+,TOD(0.0)), modarray(delta));
#else
  delta = with {
    ( [1] <= iv=[j] < [nj+1]) {
      o = output[j];
      t = target[j];
      res = o * (TOD(1.0) - o) * (t - o);
    }:res;
  }:modarray(delta);
  errsum = TOD(0.0);
#endif /* End of ERRSUM*/

  return( errsum, delta);
}

inline
DBL, DBL[.] bpnn_hidden_error_sequential(DBL[.] delta_h, DBL[.] delta_o, DBL[.,.] who, DBL[.] hidden)
{
  nh = shape(delta_h)[0]-1;
  no = shape(delta_o)[0]-1;

  errsum = TOD(0.0);
  for (j = 1; j <= nh; j++) {
    h = hidden[j];
    tmp = TOD(0.0);
    for (k = 1; k <= no; k++) {
      tmp = tmp + delta_o[k] * who[j, k];
    }
    delta_h[j] = h * (TOD(1.0) - h) * tmp;
    errsum = errsum + ABS(delta_h[j]);
  }

  return( errsum, delta_h);
}

inline
DBL, DBL[.] bpnn_hidden_error(DBL[.] delta_h, DBL[.] delta_o, DBL[.,.] who, DBL[.] hidden)
{
  nh = shape(delta_h)[0]-1;
  no = shape(delta_o)[0]-1;

#ifdef ERRSUM
  errsum, delta_h =
    with {
      ( [1] <= iv=[j] <= [nh] ) {
        h = hidden[j];
        tmp = with {
                ( [1] <= iv=[k] <= [no]) {
                  res = delta_o[k] * who[j,k];
                }:res;
              }:fold(+, TOD(0.0));
        res2 = h * (TOD(1.0) - h) * tmp;
        res1 = ABS(res2);
      }:(res1, res2);
    }:(fold(+,TOD(0.0)), modarray(delta_h));
#else
  delta_h = with {
    ( [1] <= iv=[j] <= [nh] ) {
      h = hidden[j];
      tmp = with {
              ( [1] <= iv=[k] <= [no]) {
                res = delta_o[k] * who[j,k];
              }:res;
            }:fold(+, TOD(0.0));
      res = h * (TOD(1.0) - h) * tmp;
    }:res;
  }:modarray(delta_h);
  errsum = TOD(0.0);
#endif /* End of ERRSUM*/

  return( errsum, delta_h);
}

inline
DBL[.], DBL[.,.], DBL[.,.] bpnn_adjust_weights_sequential(DBL[.] delta, DBL[.] ly, DBL[.,.] w, DBL[.,.] oldw)
{
  ndelta = shape(delta)[0] - 1;
  nly = shape(ly)[0] - 1;

  ly[0] = TOD(1.0);

  for (j = 1; j <= ndelta; j++) {
    for (k = 0; k <= nly; k++) {
      new_dw = ((ETA * delta[j] * ly[k]) + (MOMENTUM * oldw[k,j]));
      w[k, j] = w[k, j] + new_dw;
      oldw[k, j] = new_dw;
    }
  }

  return( ly, w, oldw);
}

inline
DBL[.], DBL[.,.], DBL[.,.] bpnn_adjust_weights(DBL[.] delta, DBL[.] ly, DBL[.,.] w, DBL[.,.] oldw)
{
  ndelta = shape(delta)[0] - 1;
  nly = shape(ly)[0] - 1;

/*
  ly[0] = TOD(1.0);
*/
  w, oldw = with {
              ( [0,1] <= iv=[k,j] <= [nly,ndelta]) {
                new_dw = ((ETA * delta[j] * ly[k]) + (MOMENTUM * oldw[k,j]));
                res1 = w[k, j] + new_dw;
                res2 = new_dw;
              }:(res1,res2);
            }:( modarray(w),modarray(oldw));

  return( ly, w, oldw);
}

int main()
{
  /***************** Initialization ********************/
  input_units = genarray( [IN+1], TOD(0.0));
  hidden_units = genarray( [HIDDEN+1], TOD(0.0));
  output_units = genarray( [OUT+1], TOD(0.0));

  hidden_delta = genarray( [HIDDEN+1], TOD(0.0));
  output_delta = genarray( [OUT+1], TOD(0.0));
  target = genarray( [OUT+1], TOD(0.0));

  input_weights = genarray( [IN+1, HIDDEN+1], TOD(0.0));
  hidden_weights = genarray( [HIDDEN+1, OUT+1], TOD(0.0));

  input_prev_weights = genarray( [IN+1, HIDDEN+1], TOD(0.0));
  hidden_prev_weights = genarray( [HIDDEN+1, OUT+1], TOD(0.0));

  srandom( toui(7));

  input_weights = bpnn_randomize_weights(input_weights);
  hidden_weights = bpnn_randomize_weights(hidden_weights);
  input_prev_weights = bpnn_zero_weights(input_prev_weights);
  hidden_prev_weights = bpnn_zero_weights(hidden_prev_weights);
  target = bpnn_randomize_row(target);

  input_units = load( input_units);
  /*****************************************************/
#ifdef LOOP
  for( i = 0; i < ITER; i++) {
#endif

#ifdef CUDA  /* Compile for CUDA */
    input_units, hidden_units = bpnn_layerforward_large(input_units, hidden_units,input_weights);

    hidden_units, output_units = bpnn_layerforward_sequential(hidden_units, output_units, hidden_weights);

    out_err, output_delta = bpnn_output_error_sequential(output_delta, target, output_units);

    hid_err, hidden_delta = bpnn_hidden_error_sequential(hidden_delta, output_delta, hidden_weights, hidden_units);

    hidden_units, hidden_weights, hidden_prev_weights =
      bpnn_adjust_weights_sequential(output_delta, hidden_units, hidden_weights, hidden_prev_weights);

    input_units, input_weights, input_prev_weights =
      bpnn_adjust_weights(hidden_delta, input_units, input_weights, input_prev_weights);

#else  /* Compile for sequential */

    input_units, hidden_units = bpnn_layerforward_small(input_units, hidden_units,input_weights);

    hidden_units, output_units = bpnn_layerforward_small(hidden_units, output_units, hidden_weights);

    out_err, output_delta = bpnn_output_error(output_delta, target, output_units);

    hid_err, hidden_delta = bpnn_hidden_error(hidden_delta, output_delta, hidden_weights, hidden_units);

    hidden_units, hidden_weights, hidden_prev_weights =
      bpnn_adjust_weights(output_delta, hidden_units, hidden_weights, hidden_prev_weights);

    input_units, input_weights, input_prev_weights =
      bpnn_adjust_weights(hidden_delta, input_units, input_weights, input_prev_weights);
#endif

#ifdef LOOP
  }
#endif

#ifdef OUTPUT
  for( i = 0; i < HIDDEN+1; i++) {
    for( j = 0; j < OUT+1; j++) {
      printf("%f\n", hidden_weights[i,j]);
    }
  }

  for( i = 0; i < HIDDEN+1; i++) {
    for( j = 0; j < OUT+1; j++) {
      printf("%f\n", hidden_prev_weights[i,j]);
    }
  }

  for( i = 0; i < IN+1; i++) {
    for( j = 0; j < HIDDEN+1; j++) {
      printf("%f\n", input_weights[i,j]);
    }
  }

  for( i = 0; i < IN+1; i++) {
    for( j = 0; j < HIDDEN+1; j++) {
      printf("%f\n", input_prev_weights[i,j]);
    }
  }
#else
  printf("%f\n", hidden_weights[0,0]);
  printf("%f\n", hidden_prev_weights[0,0]);
  printf("%f\n", input_weights[0,0]);
  printf("%f\n", input_prev_weights[0,0]);
#endif

  return( 0);
}

