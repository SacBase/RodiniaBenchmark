use StdIO: all;
use Array: all;
use Random: all;
use Math: all;

/*
 * The following 'defines' are used:
 *   FLOAT:  If defined, use single precision floating point data
 *   SEQ:    If defined, use sequential implemenation, i.e. for loops
 *   ERRSUM: If defined, compute error sum
 *   LOOP:   If defined, perform back propagation in a loop
 *   OUTPUT: If defeind, print entire array after computation
 */

#ifdef FLOAT
#define DBL float
#define TOD tof
#else
#define DBL double 
#define TOD tod
#endif 

#define IN     65536
#define HIDDEN 16
#define OUT    1

#define IN1     65537
#define HIDDEN1 17
#define OUT1    2

#define ITER 20

#define RAND_MAX 2147483647

#define ETA      TOD(0.3)       
#define MOMENTUM TOD(0.3)  

#define ABS(x) (((x) > TOD(0.0)) ? (x) : (-(x)))

inline
DBL[.,.] bpnn_randomize_weights( DBL[.,.] w)
{
  m = shape(w)[0];
  n = shape(w)[1];

  for (i = 0; i < m; i++) {
    for (j = 0; j < n; j++) {
     w[i,j] = TOD(random(0, RAND_MAX))/TOD(RAND_MAX);
    }
  }
  return( w);
}

inline
DBL[.] bpnn_randomize_row( DBL[.] w)
{
  m = shape(w)[0];
  for (i = 0; i <m; i++) {
    w[i] = TOD(0.1);
  }
  return( w);
}

inline
DBL[.,.] bpnn_zero_weights( DBL[.,.] w)
{
  m = shape(w)[0];
  n = shape(w)[1];

  for (i = 0; i < m; i++) {
    for (j = 0; j < n; j++) {
      w[i, j] = TOD(0.0);
    }
  }
  return( w);
}

inline
DBL[.] load( DBL[.] w)
{
  m = shape(w)[0];

  for (i = 1; i < m; i++) {
    w[i] =  TOD(random(0, RAND_MAX))/TOD(RAND_MAX);
  }
  return( w);
}

inline
DBL[.], DBL[.] bpnn_layerforward_sequential(DBL[.] l1, DBL[.] l2, DBL[.,.] conn)
{
  n1 = shape(l1)[0];
  n2 = shape(l2)[0];
/*
  l1[0] = TOD(1.0); 
*/
  for( j = 0; j < n2; j++) {
    tmp = TOD(0.0);
    for( k = 0; k < n1; k++) {
      tmp += conn[k,j]*l1[k];
    }
    l2[j] = (TOD(1.0) / (TOD(1.0) + exp(-tmp))); 
  }

  return( l1, l2);
}

inline
DBL[.], DBL[.] bpnn_layerforward_large(DBL[.] l1, DBL[.] l2, DBL[.,.] conn)
{
  n1 = shape(l1)[0];
  n2 = shape(l2)[0];

/*
  l1[0] = TOD(1.0); 
*/

  tmp = with {
          ( [0] <= iv=[k] < [n1]) {
            res = conn[k]*l1[k];
          }:res;
        }:fold(+, genarray([n2],TOD(0.0)));
 
  l2 = with {
          ([0] <= iv < shape(tmp)):(TOD(1.0) / (TOD(1.0) + exp(-tmp[iv]))); 
        }:modarray(tmp);

  return( l1, l2);
}

inline
DBL[.], DBL[.] bpnn_layerforward_small(DBL[.] l1, DBL[.] l2, DBL[.,.] conn)
{
  n1 = shape(l1)[0];
  n2 = shape(l2)[0];
/*
  l1[0] = TOD(1.0); 
*/
  l2 = with {
         ( [0] <= iv=[j] < [n2]) {
           tmp = with {
                   ( [0] <= iv=[k] < [n1]):conn[k, j]*l1[k];
                 }:fold(+, TOD(0.0));
           res = (TOD(1.0) / (TOD(1.0) + exp(-tmp))); 
         }:res;
       }:modarray(l2); 

  return( l1, l2);
}

inline
DBL, DBL[.] bpnn_output_error_sequential(DBL[.] delta, DBL[.] target, DBL[.] output)  
{
  nj = shape(delta)[0]; 

  errsum = TOD(0.0);
  for (j = 0; j < nj; j++) {
    o = output[j];
    t = target[j];
    delta[j] = o * (TOD(1.0) - o) * (t - o);
    errsum = errsum + ABS(delta[j]);
  }

  return( errsum, delta);
}

inline
DBL, DBL[.] bpnn_output_error(DBL[.] delta, DBL[.] target, DBL[.] output)  
{
  nj = shape(delta)[0]; 

#ifdef ERRSUM
  errsum, delta = 
    with {
      ( [0] <= iv=[j] < [nj]) {
        o = output[j];
        t = target[j];
        res2 = o * (TOD(1.0) - o) * (t - o);
        res1 = ABS( res2);
      }:(res1, res2);
    }:(fold(+,TOD(0.0)), modarray(delta));   
#else
  delta = with {
    ( [0] <= iv=[j] < [nj]) {
      o = output[j];
      t = target[j];
      res = o * (TOD(1.0) - o) * (t - o);
    }:res;
  }:modarray(delta); 
  errsum = TOD(0.0);
#endif /* End of ERRSUM*/

  return( errsum, delta);
}

inline
DBL, DBL[.] bpnn_hidden_error_sequential(DBL[.] delta_h, DBL[.] delta_o, DBL[.,.] who, DBL[.] hidden)
{
  nh = shape(delta_h)[0]; 
  no = shape(delta_o)[0]; 

  errsum = TOD(0.0);
  for (j = 0; j < nh; j++) {
    h = hidden[j];
    tmp = TOD(0.0);
    for (k = 0; k < no; k++) {
      tmp = tmp + delta_o[k] * who[j, k];
    }
    delta_h[j] = h * (TOD(1.0) - h) * tmp;
    errsum = errsum + ABS(delta_h[j]);
  }

  return( errsum, delta_h);
}

inline
DBL, DBL[.] bpnn_hidden_error(DBL[.] delta_h, DBL[.] delta_o, DBL[.,.] who, DBL[.] hidden)
{
  nh = shape(delta_h)[0]; 
  no = shape(delta_o)[0]; 

#ifdef ERRSUM
  errsum, delta_h = 
    with {
      ( [0] <= iv=[j] < [nh] ) {
        h = hidden[j];
        tmp = with {
                ( [0] <= iv=[k] < [no]) {
                  res = delta_o[k] * who[j,k];
                }:res;
              }:fold(+, TOD(0.0));
        res2 = h * (TOD(1.0) - h) * tmp; 
        res1 = ABS(res2);
      }:(res1, res2);      
    }:(fold(+,TOD(0.0)), modarray(delta_h));
#else
  delta_h = with {
    ( [0] <= iv=[j] < [nh] ) {
      h = hidden[j];
      tmp = with {
              ( [0] <= iv=[k] < [no]) {
                res = delta_o[k] * who[j,k];
              }:res;
            }:fold(+, TOD(0.0));
      res = h * (TOD(1.0) - h) * tmp; 
    }:res;      
  }:modarray(delta_h);
  errsum = TOD(0.0);
#endif /* End of ERRSUM*/

  return( errsum, delta_h);
}

inline
DBL[.], DBL[.,.], DBL[.,.] bpnn_adjust_weights_sequential(DBL[.] delta, DBL[.] ly, DBL[.,.] w, DBL[.,.] oldw)
{
  ndelta = shape(delta)[0];
  nly = shape(ly)[0];
/*
  ly[0] = TOD(1.0);
*/
  for (j = 0; j < ndelta; j++) {
    for (k = 0; k < nly; k++) {
      new_dw = ((ETA * delta[j] * ly[k]) + (MOMENTUM * oldw[k,j]));
      w[k, j] = w[k, j] + new_dw;
      oldw[k, j] = new_dw;
    }
  }

  return( ly, w, oldw);
}

inline
DBL[.], DBL[.,.], DBL[.,.] bpnn_adjust_weights(DBL[.] delta, DBL[.] ly, DBL[.,.] w, DBL[.,.] oldw)
{
  ndelta = shape(delta)[0];
  nly = shape(ly)[0];

/*
  ly[0] = TOD(1.0);
*/
  w, oldw = with {
              ( [0,0] <= iv=[k,j] < [nly,ndelta]) {
                new_dw = ((ETA * delta[j] * ly[k]) + (MOMENTUM * oldw[k,j]));
                res1 = w[k, j] + new_dw;
                res2 = new_dw;
              }:(res1,res2);
            }:( modarray(w),modarray(oldw));

  return( ly, w, oldw);
}

int main()
{
  /***************** Initialization ********************/
  input_units = genarray( [IN], TOD(0.0));
  hidden_units = genarray( [HIDDEN], TOD(0.0));
  output_units = genarray( [OUT], TOD(0.0));

  hidden_delta = genarray( [HIDDEN], TOD(0.0));
  output_delta = genarray( [OUT], TOD(0.0));
  target = genarray( [OUT], TOD(0.0));

  input_weights = genarray( [IN, HIDDEN], TOD(0.0));
  hidden_weights = genarray( [HIDDEN, OUT], TOD(0.0));

  input_prev_weights = genarray( [IN, HIDDEN], TOD(0.0));
  hidden_prev_weights = genarray( [HIDDEN, OUT], TOD(0.0));

  srandom( 7);  

  input_weights = bpnn_randomize_weights(input_weights);
  hidden_weights = bpnn_randomize_weights(hidden_weights);
  input_prev_weights = bpnn_zero_weights(input_prev_weights);
  hidden_prev_weights = bpnn_zero_weights(hidden_prev_weights);
  target = bpnn_randomize_row(target);

  input_units = load( input_units);
  /*****************************************************/
#ifdef LOOP
  for( i = 0; i < ITER; i++) {
#endif

#ifdef CUDA
    input_units, hidden_units = bpnn_layerforward_large(input_units, hidden_units,input_weights);

    hidden_units, output_units = bpnn_layerforward_sequential(hidden_units, output_units, hidden_weights);

    out_err, output_delta = bpnn_output_error_sequential(output_delta, target, output_units);

    hid_err, hidden_delta = bpnn_hidden_error_sequential(hidden_delta, output_delta, hidden_weights, hidden_units); 

    hidden_units, hidden_weights, hidden_prev_weights = 
      bpnn_adjust_weights_sequential(output_delta, hidden_units, hidden_weights, hidden_prev_weights);

    input_units, input_weights, input_prev_weights = 
      bpnn_adjust_weights(hidden_delta, input_units, input_weights, input_prev_weights);

#else

    input_units, hidden_units = bpnn_layerforward_small(input_units, hidden_units,input_weights);

    hidden_units, output_units = bpnn_layerforward_small(hidden_units, output_units, hidden_weights);

    out_err, output_delta = bpnn_output_error(output_delta, target, output_units);

    hid_err, hidden_delta = bpnn_hidden_error(hidden_delta, output_delta, hidden_weights, hidden_units); 

    hidden_units, hidden_weights, hidden_prev_weights = 
      bpnn_adjust_weights(output_delta, hidden_units, hidden_weights, hidden_prev_weights);

    input_units, input_weights, input_prev_weights = 
      bpnn_adjust_weights(hidden_delta, input_units, input_weights, input_prev_weights);

#endif

#ifdef LOOP
  }
#endif

#ifdef OUTPUT
  for( i = 0; i < HIDDEN; i++) {
    for( j = 0; j < OUT; j++) {
      printf("%f\n", hidden_weights[i,j]);
    }
  }

  for( i = 0; i < HIDDEN; i++) {
    for( j = 0; j < OUT; j++) {
      printf("%f\n", hidden_prev_weights[i,j]);
    }
  }

  for( i = 0; i < IN; i++) {
    for( j = 0; j < HIDDEN; j++) {
      printf("%f\n", input_weights[i,j]);
    }
  }

  for( i = 0; i < IN; i++) {
    for( j = 0; j < HIDDEN; j++) {
      printf("%f\n", input_prev_weights[i,j]);
    }
  }
#else
  printf("%f\n", hidden_weights[0,0]);
  printf("%f\n", hidden_prev_weights[0,0]);
  printf("%f\n", input_weights[0,0]);
  printf("%f\n", input_prev_weights[0,0]);
#endif

  return( 0);
}

